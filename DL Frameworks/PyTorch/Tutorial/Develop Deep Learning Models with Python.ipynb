{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the Data\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to prepare numerical input data.\n",
    "\n",
    "We will use pandas to load the data and toold from scikit-learn to perform encoding.\n",
    "\n",
    "There is dataset class provided by PyTorch, which can be used to customize and load our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skeleton of custom Dataset class\n",
    "\n",
    "# dataset definition\n",
    "class CSVDataset(Dataset):\n",
    "    # load the dataset\n",
    "    def __init__(self, path):\n",
    "        # store the inputs and outputs\n",
    "        self.X = ...\n",
    "        self.y = ...\n",
    " \n",
    "    # number of rows in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    " \n",
    "    # get a row at an index\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading, there is an instance (DataLoader class) provided during training and evaluation of our model.\n",
    "This DataLoader instance can be created for training, testing or validation datasets.\n",
    "\n",
    "To split the dataset there is an inhouse function `random_split()`. \n",
    "DataLoader also accepts batch size and shuffling (even with every epoch).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample\n",
    "# create the dataset\n",
    "dataset = CSVDataset(...)\n",
    "# select rows from the dataset\n",
    "train, test = random_split(dataset, [[...], [...]])\n",
    "# create a data loader for train and test sets\n",
    "train_dl = DataLoader(train, batch_size=32, shuffle=True)\n",
    "test_dl = DataLoader(test, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once defined, a DataLoader can be enumerated, yielding one batch worth of samples each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "# train the model\n",
    "for i, (inputs, targets) in enumerate(train_dl):\n",
    "    ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the model\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Defining model refers to defining a class which extends th Module class (https://pytorch.org/docs/stable/nn.html#module)\n",
    "\n",
    "- The constructor of your class defines the layers of the model and the forward() function is the override that defines how to forward propagate input through the defined layers of the model.\n",
    "\n",
    "- Many layers are available, such as \n",
    "    - `Linear` for fully connected layers,\n",
    "    - `Conv2d` for convolutional layers,\n",
    "    - `MaxPool2d` for pooling layers.\n",
    "\n",
    "\n",
    "- Activation functions can also be defined as layers, such as ReLU, Softmax, and Sigmoid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  a simple MLP model with one layer\n",
    "# model definition\n",
    "class MLP(Module):\n",
    "    # define model elements\n",
    "    def __init__(self, n_inputs):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer = Linear(n_inputs, 1)\n",
    "        self.activation = Sigmoid()\n",
    " \n",
    "    # forward propagate input\n",
    "    def forward(self, X):\n",
    "        X = self.layer(X)\n",
    "        X = self.activation(X)\n",
    "        return X\n",
    "\n",
    "#The weights of a given layer can also be initialized after the layer is defined in the constructor.\n",
    "#Common examples include the Xavier and He weight initialization schemes. For example:\n",
    "\n",
    "xavier_uniform_(self.layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training process requires us to define a loss function and an optimization algorithm.\n",
    "Some common loss functions are:\n",
    "- BCELoss: Binary cross-entropy loss for binary classification.\n",
    "- CrossEntropyLoss: Categorical cross-entropy loss for multi-class classification.\n",
    "- MSELoss: Mean squared loss for regression.\n",
    "\n",
    "More function can be found here: [Loss and Loss Functions for Training Deep Learning Neural Networks](https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/)\n",
    "\n",
    "Stochastic gradient descent is used for optimization, and is the standard algorithm provided by [SGD class](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD) (default). We can also use [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimization\n",
    "criterion = MSELoss()\n",
    "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training models involve enumerating `DataLoader` for training dataset.\n",
    "\n",
    "Firstly, a loop is required for no. of training epochs. Then inner loop is required for the mini-batches gor stochastic gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "# enumerate epochs\n",
    "for epoch in range(100):\n",
    "    # enumerate mini batches\n",
    "    for i, (inputs, targets) in enumerate(train_dl):\n",
    "    \t..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each update to the model involves the same general pattern comprised of:\n",
    "\n",
    "- Clearing the last error gradient.\n",
    "- A forward pass of the input through the model.\n",
    "- Calculating the loss for the model output.\n",
    "- Backpropagating the error through the model.\n",
    "- Update the model in an effort to reduce loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear the gradients\n",
    "optimizer.zero_grad()\n",
    "# compute the model output\n",
    "yhat = model(inputs)\n",
    "# calculate loss\n",
    "loss = criterion(yhat, targets)\n",
    "# credit assignment\n",
    "loss.backward()\n",
    "# update model weights\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is `fit` we can evaluate its performance using test dataset. And we can still use `DataLoader` for test dataset and collect predictions for test set, then compare the predictions to the expected values of the test set and calculating a performace metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (inputs, targets) in enumerate(test_dl):\n",
    "    # evaluate the model on the test set\n",
    "    yhat = model(inputs)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ***fit model*** can be used to make predictions on new data, i.e. you can use either single image or single row of data to make prediction. However, this requires you to wrap the data in PyTorch Tensor data structure.\n",
    "\n",
    "A **Tensor** is just the PyTorch version of a NumPy array for holding data. It also allows you to perform the automatic differentiation tasks in the model graph, like calling `backward()` when training the model. The prediction will be Tensor too, although you can retrieve NumPy array by [detaching the Tensor](https://pytorch.org/docs/stable/autograd.html#torch.Tensor.detach) from the automatic differentiation graph and calling the NumPy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "# convert row to data\n",
    "row = Variable(Tensor([row]).float())\n",
    "# make prediction\n",
    "yhat = model(row)\n",
    "# retrieve numpy array\n",
    "yhat = yhat.detach().numpy()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
